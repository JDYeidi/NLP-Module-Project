# -*- coding: utf-8 -*-
"""NLP_MR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mWS0e4lV6YLkXhkANtbrMN-zNciiMNRR

Task 1: Out of the Box Sentiment Analysis
"""

# HIDE OUTPUT
!pip install transformers
!pip install transformers[sentencepiece]
!pip install datasets
!pip install google-cloud-translate

#Import libraries
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

#Preparing model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")
tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")

classifier = pipeline("sentiment-analysis", model = model, tokenizer = tokenizer)

with open('tiny_movie_reviews_dataset.txt', 'r') as f:
    reviews = f.readlines()
c = 0
for i in range(len(reviews)):
    inputs = reviews[i]
    output = classifier(inputs, max_length=512, truncation=True)
    if output[0]['label'] == 'LABEL_0':
        print("Review " + str(c) + ": " + "Negative")
    elif output[0]['label'] == 'LABEL_1':
        print("Review " + str(c) + ": " + "Neutral")
    elif output[0]['label'] == 'LABEL_2':
        print("Review " + str(c) + ": " + "Positive")
    c += 1

"""Task 2: Take a basic, pretrained NER model, and train further on a task-specific dataset"""

from datasets import load_dataset
from transformers import AutoTokenizer
from transformers import DefaultDataCollator
import tensorflow as tf
from transformers import TFAutoModelForSequenceClassification
import keras
from matplotlib import pyplot as plt

class ModelTrainer():
    def __init__(self, samples_data_test, samples_data_train):

      #samples of dataset
      self.samples_data_train = samples_data_train
      self.samples_data_test = samples_data_test

      #Loading dataset
      self.emotions = load_dataset("emotion")

      #Data collator
      self.data_collator = DefaultDataCollator(return_tensors="tf")

    def tokenize(self,rows):
      return self.tokenizer(rows['text'], padding="max_length", truncation=True)

    def split_dataset(self,data_set):
      self.small_train_dataset = data_set["train"].shuffle(seed=42).select([i for i in list(range(self.samples_data_train))])
      self.small_eval_dataset = data_set["test"].shuffle(seed=42).select([i for i in list(range(self.samples_data_test))])

    def tf_datasets(self):
      self.tf_train_dataset = self.small_train_dataset.to_tf_dataset(
          columns=["attention_mask", "input_ids"],
          label_cols=["labels"],
          shuffle=True,
          collate_fn=self.data_collator,
          batch_size=8,
      )

      self.tf_validation_dataset = self.small_eval_dataset.to_tf_dataset(
          columns=["attention_mask", "input_ids"],
          label_cols=["labels"],
          shuffle=False,
          collate_fn=self.data_collator,
          batch_size=8,
      )


    def train_model(self):
      self.model = TFAutoModelForSequenceClassification.from_pretrained(\
                                                                        "distilbert-base-uncased", num_labels=6)
      self.model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=tf.metrics.SparseCategoricalAccuracy(),
        )

      self.history = self.model.fit(self.tf_train_dataset, validation_data=self.tf_validation_dataset,epochs=5)
      return self.model
      
    
    def main(self):
      self.model_ckpt = "distilbert-base-uncased"
      self.tokenizer = AutoTokenizer.from_pretrained(self.model_ckpt)
      self.emotions.set_format(type=None)
      self.tokenized_datasets = self.emotions.map(self.tokenize, batched=True)
      self.split_dataset(self.tokenized_datasets)
      self.tf_datasets()
      self.modelh = self.train_model()

    def graphic_model(self):
      plt.plot(self.history.history['loss'])
      plt.plot(self.history.history['val_loss'])
      plt.title('model loss')
      plt.ylabel('loss')
      plt.xlabel('epoch')
      plt.legend(['train', 'val'], loc='upper left')
      plt.show()

      plt.plot(self.history.history['sparse_categorical_accuracy'])
      plt.plot(self.history.history['val_sparse_categorical_accuracy'])
      plt.title('model accuracy')
      plt.ylabel('accuracy')
      plt.xlabel('epoch')
      plt.legend(['train', 'val'], loc='upper left')
      plt.show()

if __name__ == '__main__':

	samples_train = 3000
	samples_test = 300
	model_trainer = ModelTrainer(samples_test, samples_train)
	model_trainer.main()
	model_trainer.graphic_model()

"""Task 3:Set up and compare model performance of two different translation models"""

from google.cloud import translate_v2
import requests, uuid, json
from nltk.translate.bleu_score import sentence_bleu
import numpy as np
import statistics

class translate_blue():
  def __init__(self, lang1_set, lang2_set, lang_from, lang_to, key_azure, region_azure, gcp_keys_json_name):
    self.lang1 = lang1_set
    self.lang2 = lang2_set
    self.lang_from = lang_from
    self.lang_to = lang_from
    self.cod_key = key_azure
    self.cod_region = region_azure
    self.gcp_keys_json_name = gcp_keys_json_name

  def azure_translate(self,text):
    # Add your key and endpoint
    endpoint = "https://api.cognitive.microsofttranslator.com"

    path = '/translate'
    constructed_url = endpoint + path

    params = {
        'api-version': '3.0',
        'from': self.lang_from,
        'to': self.lang_to
    }

    headers = {
        'Ocp-Apim-Subscription-Key': self.cod_key,
        # location required if you're using a multi-service or regional (not global) resource.
        'Ocp-Apim-Subscription-Region': self.cod_region,
        'Content-type': 'application/json',
        'X-ClientTraceId': str(uuid.uuid4())
    }

    # You can pass more than one object in body.
    body = [{
        'text': text
    }]

    request = requests.post(constructed_url, params=params, headers=headers, json=body)
    response = request.json()

    return response[0]["translations"][0]["text"]

  def gcp_translate(self,text):
    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = self.gcp_keys_json_name
    client = translate_v2.Client()
    output = client.translate(text, self.lang_to)
    return output['translatedText']

  def main(self):
    points_gcp = []
    points_azure = []
    with open(self.lang1, 'r') as f:
      lang1 = f.readlines()

    with open(self.lang2, 'r') as f:
      lang2 = f.readlines()

    for i in range(len(lang1)):
      output_azure = self.azure_translate(lang1[i])
      #print(output_azure)
      blue_azure = sentence_bleu(lang2[i].split(), output_azure.split())
      points_azure.append(blue_azure)

      output_gcp = self.gcp_translate(lang1[i])
      #print(output_gcp)
      blue_gcp = sentence_bleu(lang2[i].split(), output_gcp.split())
      points_gcp.append(blue_azure)

    print("Blue Score GCP: " + str(statistics.mean(points_gcp)))
    print("Blue Score azure: " + str(statistics.mean(points_azure)))

if __name__ == '__main__':
  lang1_set = 'en_corpus.txt'
  lang2_set = 'es_corpus.txt'
  lang_from = 'en'
  lang_to = 'es'
  cod_key = 'AzureKey'
  cod_region = 'AzureRegion'
  gcp_keys = 'private_key.json'
  
  blue_score = translate_blue(lang1_set, lang2_set, lang_from, lang_to, cod_key, cod_region, gcp_keys)
  blue_score.main()
